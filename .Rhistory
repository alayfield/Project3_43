theta <- atan(y/x) * (180/pi)
theta[0:5]
his(theta, frequency=FALSE)
hist(theta, frequency=FALSE)
hist(theta)
hist(theta, frequency=FALSE, main="")
mu <- mean(theta)
sigma <- sd(theta)
quantiles <- qnorm(c(0.20, 0.80), mean = mu, sd = sigma)
x <- runif(100000) ; y <- runif(100000)
x <- runif(100000) ; y <- runif(100000)
theta <- atan(y / x) * (180 / pi)
x <- runif(100000) ; y <- runif(100000)
set.seed(010)
set.seed(010)
x <- runif(100000) ; y <- runif(100000)
theta <- atan(y / x) * (180 / pi)
# Question 2
# a
hist(theta, frequency=FALSE, main="")
# Question 3
# a
mu <- mean(theta)
# b
sigma <- sd(theta)
# c
quantiles <- qnorm(c(0.20, 0.80), mean = mu, sd = sigma)
x <- runif(100000) ; y <- runif(100000)
theta <- atan(y / x) * (180 / pi)
# Question 2
# a
hist(theta, frequency=FALSE, main="")
# Question 3
# a
mu <- mean(theta)
# b
sigma <- sd(theta)
# c
quantiles <- qnorm(c(0.20, 0.80), mean = mu, sd = sigma)
#Question 4
set.seed(010)
x <- runif(100000) ; y <- runif(100000)
theta <- atan(y / x) * (180 / pi)
# Question 2
# a
hist(theta, frequency=FALSE, main="")
# Question 3
# a
mu <- mean(theta)
# b
sigma <- sd(theta)
# c
quantiles <- qnorm(c(0.20, 0.80), mean = mu, sd = sigma)
#Question 4
#Question 4
#Question 4
Y <- c(FALSE, FALSE TRUE FALSE)
y <- c(FALSE, FALSE, TRUE, FALSE)
all(xor(x, y) & (x & y))
x <- c(TRUE, TRUE, FALSE, FALSE)
y <- c(FALSE, FALSE, TRUE, FALSE)
all(xor(x, y) & (x & y))
update.packages(ask = FALSE)
update.packages(ask = FALSE)
matrix(1:6, nrow = 2)
cbind(1:3, 3:1)
rbind(1:3, 3:1)
X <- matrix(1:4, nrow = 2)
X
apply(X, 2, prod)
X <- matrix(1:6, ncol = 2) ; Y <- matrix(1:6, ncol = 2)
X * Y
dim(X * Y)
dim(x)
dim(X)
X <- matrix(1:6, ncol = 2) ; Y <- matrix(1:6, nrow = 2)
X %*% Y
X
Y
X <- matrix(1:4, nrow = 2)
diag(solve(X) %*% X)
solve(X)
x <- c(1, 2, 3, 4, "a", "b", "c")
y <- c(5, 6, 7, 8, "d", "e", "f")
x + y
z <- c(x, y)
f <- c(c(x), c(y))
f
list(x, y)
f <- list(x, y)
f
f[1]
f[1][1]
f[[1]][1]
f[[2]][4]
f[[1]][c(1, 3, 5)]
x <- c(1, 1, 0, 0, 1)
y <- c(0, 1, 1, 1, 0)
all(xor(x, y) & (x & y))
x <- list(title = "The Big Lebowski", rottentomatoes = c(tomatometer = 82, audience = 94))
x[["rottentomatoes"]][1]
x["rottentomatoes"][[1]][1]
x["rottentomatoes"][[1]]
x["rottentomatoes"][1]
x$rottentomatoes[1]
array(1:3, c(2,4))
f <- array(1:3, c(2,4))
select(f, 1, 2)
f[, c(3, 1, 2)]
g <- array(1:3, c(5,4))
g
h <- as.data.frame(g, row.names = c("a", "b", "c", "d"))
h
arrage(h, V3)
h[sort(h$V2), ]
h <- as.data.frame(g)
h
h <- as.data.frame(g, row.names = c("a", "b", "c", "d", "e"))
h
h[sort(h$V2), ]
h[order(h$V2), ]
sort(h$V2)
h[[V2]][2] <- NA
h[[2]][2] <- NA
h
filter(h, is.na(V2))
subset(h, is.na(V2))
subset(h, !is.na(V2))
h[is.na(h$V2), ]
filter(h, is.na(V2))
h$lp100k <- h |> transform(lp100k = (100 * 3.785 / 1.61) / V4)
h
g <- h |> transform(lp100k = (100 * 3.785 / 1.61) / V4)
g
lp100k <- transform(h, lp100k = (100 * 3.785 / 161) / V4)
h
g <- array(1:3, c(5,4))
h <- as.data.frame(g, row.names = c("a", "b", "c", "d", "e"))
h
lp100k <- transform(h, lp100k = (100 * 3.785 / 161) / V4)
h
lp100k <- mutate(h, lp100k = (100 * 3.785 / 161) / V4)
j <- h |> transform(lp100k = (100 * 3.785 / 1.61) / V4)
j
import dbplyr
lp100k <- dplyr::mutate(h, lp100k = (100 * 3.785 / 1.61) / mpg)
lp100k <- dplyr::mutate(h, lp100k = (100 * 3.785 / 1.61) / V4)
h
lp100k
h$lp100k <- h |> transform(lp100k = (100 * 3.785 / 1.61) / V4)
h
spdata <- read.csv("http://www.stat.ufl.edu/~winner/data/bioequiv_sulf.csv")
spdata.AS <- spdata[measure==2 & drug==1,]
attach(spdata.AS)
spdata <- read.csv("http://www.stat.ufl.edu/~winner/data/bioequiv_sulf.csv")
attach(spdata); names(spdata)
spdata
spdata.AS <- spdata[measure==2 & drug==1,]
detach(spdata)
attach(spdata.AS)
AUC.sulf <- y[measure==2 & drug==1]    # measure=AUC, drug=sulfadoxine
form.AUC.sulf <- form[measure==2 & drug==1] # form=1 if Test, form=2 if Ref
form.AUC.sulf <- factor(form.AUC.sulf, levels=1:2, labels=c("T","R"))
cbind(form.AUC.sulf, AUC.sulf)
## Side-by-side Boxplots
plot(AUC.sulf ~ form.AUC.sulf, main="Sulfadoxine AUC by Formulation")
source("~/Downloads/hw1_AUC.R")
X <- rep(seq(130,180,10), each = 2)
X <- rep(seq(130,180,10), each = 2)
Y <- c(21.28, 22.11, 16.08, 23.16, 20.67, 19.66, 16.93, 17.62, 18.11, 19.30, 12.76, 11.89)
reg <- lm(Y ~ X)
summary(reg)
beta0 <- reg$coef[1]
beta1 <- reg$coef[2]
Y[1]  ## Q1 part 2
beta0
beta1
Yhat1 <- beta0 + Y[1] * beta1
Yhat1 ## Q1 part 3
X[1]
Yhat1 <- beta0 + X[1] * beta1
Yhat1 ## Q1 part 3
e1 <- Y1 - Yhat1
e1 <- Y[1] - Yhat1
e1
Yhat <- beta0 + X * beta1
sum(Y - Yhat)
Yhat
sum((Y - Yhat)^2)
sse <- sum((fitted(reg) - Y)^2)
sse
sse / 10
SSxx <- sum((X - mean(X))^2)
SSxx
xx
sb1 <- mse / ssxx
X <- rep(seq(130,180,10), each = 2)
Y <- c(21.28, 22.11, 16.08, 23.16, 20.67, 19.66, 16.93, 17.62, 18.11, 19.30, 12.76, 11.89)
reg <- lm(Y ~ X)
summary(reg)
beta0 <- reg$coef[1]
beta1 <- reg$coef[2]
Y[1]  ## Q1 part 2
X[1]
beta0
beta1
Yhat1 <- beta0 + X[1] * beta1
Yhat1 ## Q1 part 3
e1 <- Y[1] - Yhat1
e1 ## Q1 part 4
Yhat <- beta0 + X * beta1
sum((Y - Yhat)^2)
sse <- sum((fitted(reg) - Y)^2)
sse
mse <- sse / 10
ssxx <- sum((X - mean(X))^2)
sb1 <- mse / ssxx
sb1
sb1 <- (mse / ssxx)^(1/2)
sb1
t95 <- qt(0.975, 10)
sb1 <- reg$coef[4]
sb1
sb1 <- reg$coef[3]
sb1
sb1 <- reg$coef[2]
sb1
sb1 <- reg$coef[[4]]
sb1 <- reg$coef[6]
sb1
sb1 <- (mse / ssxx)^(1/2)
b1.LB <- beta1 - t95 * sb1
b1.UB <- beta1 + t95 * sb1
b1.LB
b1.UB
sb0 <- (mse(1/10 + (mean(X)^2)/ssxx))^(1/2)
sb0 <- (mse * (1/10 + (mean(X)^2)/ssxx))^(1/2)
sb0
anova(reg)
sb0 <- (mse * (1/12 + (mean(X)^2)/ssxx))^(1/2)
sb0
b0.LB <- beta0 - t95 * sb0 ## Q4
b0.UB <- beta0 + t95 * sb0 ## Q4
b0.LB
B0.UB
b0.UB
Yhat150 <- beta0 + 150 * beta1
Yhat150
Xbar <- mean(X)
sYhat150 <- (mse * (1/12 + ((150 - Xbar)^2)/ssxx))^(1/2)
sYhat150
Yhat150.LB <- Yhat150 - t95 * sYhat150 ## Q5
Yhat150.UB <- Yhat150 + t95 * sYhat150 ## Q5
Yhat150.LB
Yhat150.UB
spred150 <- (mse * (1 + 1/12 + ((150 - Xbar)^2)/ssxx))^(1/2)
pred150.LB <- Yhat150 - t95 * spred150 ## Q6
pred150.UB <- Yhat150 + t95 * spred150 ## Q6
pred150.LB
pred150.UB
corr(Y ~ X)
cor(Y ~ X)
cor(X, Y)
r <- cor(X, Y)
r^2
cor.test(X, Y)
t95
library(mvtnorm)
install.packages("mvtnorm", repos="http://R-Forge.R-project.org")
library(mvtnorm)
library(e1071)
library(class)
## sample size and covariate dimension ranges
nVec = c(100,500,1000,3000)
pVec = c(1,3,5,10,15,20,25,30,40,50,75,100)
## store results
ratio = matrix(NA, length(pVec), length(nVec))
for (ni in 1 : length(nVec)) {
for (pi in 1 : length(pVec)) {
n = nVec[ni]
p = pVec[pi]
## generate covariate matrix
x = matrix(rnorm(n*p), n, p)
## get distances between all pairs of points
distMat = as.matrix(dist(x))
## extract just upper triangle entries to avoid repeats
distances = as.vector(distMat[which(upper.tri(distMat) == TRUE)])
minDist = min(distances)
maxDist = max(distances)
## store ratio of max to min
ratio[pi,ni] = maxDist / minDist
}
}
## plot the effect of p on this ratio for different sample sizes
plot(pVec, ratio[,1], type="l", ylim=c(1,50), lwd=3,
xlab = "# of covariates", ylab="Ratio")
lines(pVec, ratio[,2], lwd=3, col=2)
lines(pVec, ratio[,3], lwd=3, col=3)
lines(pVec, ratio[,4], lwd=3, col=4)
abline(h = 1, lty=2, lwd=2)
legend("topright", paste("n =", nVec), col=1:4, lwd=3, lty=1)
## Can also look at the histogram of differences
n = 500
p = 5
x = matrix(rnorm(n*p), n, p)
## get distances between all pairs of points
distMat = as.matrix(dist(x))
## extract just upper triangle entries to avoid repeats
distances = as.vector(distMat[which(upper.tri(distMat) == TRUE)])
hist(distances)
## More predictors now
n = 500
p = 2500
x = matrix(rnorm(n*p), n, p)
## get distances between all pairs of points
distMat = as.matrix(dist(x))
## extract just upper triangle entries to avoid repeats
distances = as.vector(distMat[which(upper.tri(distMat) == TRUE)])
hist(distances)
## Now see what happens if we do dimension reduction
set.seed(2)
n = 1000
p = 200
x = matrix(rnorm(n*p), n, p)
## true regression function
f = function(x) {
return(-1.5 + exp(x[,1]) + 2*x[,2] - log(x[,3]^2))
}
## generate outcome
y = rbinom(n, 1, p = pnorm(f(x)))
## generate testing data
xtest = matrix(rnorm(500*p), 500, p)
ytest = rbinom(n, 1, p = pnorm(f(xtest)))
## Find the 5 covariates with largest correlation with y
keep = order(abs(cor(y, x)), decreasing=TRUE)[1:5]
x2 = x[,keep]
xtest2 = xtest[,keep]
## run knn on full set
knnMod = knn(train=x, test=xtest, k=10, cl=y)
knnPred = as.numeric(as.character(knnMod))
## compare predictions with truth
mean(knnPred != ytest)
## now run only on the chosen covariates
knnMod5 = knn(train=x2, test=xtest2, k=10, cl=y)
knnPred5 = as.numeric(as.character(knnMod5))
## compare predictions with truth
mean(knnPred5 != ytest)
## now run on the true set of covariates
x3  = x[,1:3]
xtest3 = xtest[,1:3]
knnModTrue = knn(train=x3, test=xtest3, k=10, cl=y)
knnPredTrue = as.numeric(as.character(knnModTrue))
## compare predictions with truth
mean(knnPredTrue != ytest)
library(tree)
library(tree)
library(randomForest)
install.packages("randomForest")
library(randomForest)
library("randomForest")
setwd("~/Documents/COP3530/Project3")
library(dplyr)
library(dplyr)
library(stringr)
## Removed commas in strings
dat2 <- read.csv("csv/cleaned_tracks.csv")
setwd("~/Documents/COP3530/Project3")
setwd("~/Documents/COP3530/Project3_43")
## Removed commas in strings
dat2 <- read.csv("csv/cleaned_tracks.csv")
dat2 <- dat2[!grepl("Live at ", dat2$album, ignore.case=TRUE),]
n_distinct(dat2$album_id)
albumIDs <- names(table(dat2$album_id))
dat2[which(dat2$album_id == albumIDs[1]),]
n_distinct(dat2[dat2$album_id == albumIDs[1],]$artists)
(n_distinct(dat2[which(dat2$album_id == albumIDs[1]),]$artists) > 1)
n_distinct(dat2[dat2$album_id == albumIDs[1],]$artists) > 1
albumIDs2 <- albumIDs
apply(albumIDs, 2, n_distinct(dat2[dat$album_id == .,]$artists))
apply(albumIDs, 2, n_distinct(dat2[dat2$album_id == .,]$artists))
multiArtist <- function(x) {
n_distinct(dat2[dat2$album_id == x,]$artists)
}
apply(albumIDs2, 2, multiArtist)
apply(albumIDs2, 1, multiArtist)
lapply(albumIDs2, 1, multiArtist)
lapply(albumIDs2, multiArtist)
## Removed commas in strings
dat2 <- read.csv("csv/cleaned_tracks.csv")
dat2 <- dat2[!grepl("Live at ", dat2$album, ignore.case=TRUE),]
groupedData <- group_by(dat2, album_id) |>
numArtists <- summarize(groupedData, num_artists = n_distinct(dat2$artists))
groupedData <- group_by(dat2, album_id)
numArtists <- summarize(groupedData, num_artists = n_distinct(dat2$artists))
## Removed commas in strings
dat2 <- read.csv("csv/cleaned_tracks.csv")
dat2 <- dat2[!grepl("Live at ", dat2$album, ignore.case=TRUE),]
groupedData <- group_by(dat2, album_id)
numArtists <- summarize(groupedData, num_artists = n_distinct(dat2$artists))
collabAlbum
numArtists
groupedData
groupedData <- dat2 |> group_by(album_id)
groupedData
groupedData <- dat2 |> group_by(album_id) |>
summarize(num_artists = n_distinct(dat2$artists))
artistCount <- table(df2$album_id, df2$artists)
artistCount <- table(dat2$album_id, dat2$artists)
artistCount
artistCount <- table(dat2$artists, dat2$album_id)
n_distinct(dat2$album_id)
n_distinct(dat2$artists)
n_distinct(dat2$artist_ids)
dat2$album_id
albumIDs <- dat2$album_id
dat2[albumIDs[1],]
dat2$albumIDs[albumIDs[1],]
dat2$albumIDs==albumIDs[1]
dat2[dat2$albumIDs == albumIDs[1],]
dat2[which(dat2$albumIDs == albumIDs[1]),]
pull(dat2, albumID[1])
summarize(dat2$album_id, num_artists = n_distinct(artists))
artist <- dat2$artists
n_distinct(artist)
count(dat2, artists, album_id)
artistCount <- count(dat2, artists, album_id)
artistCount$album_id[artistCount$n == 1]
artistCount$album_id[artistCount$n > 1]
soloAlbum <- artistCount$album_id[artistCount$n > 1]
dat2[which(dat2$album_id %in% soloAlbum),]
n_distinct(soloAlbum$album_id)
n_distinct(soloAlbum)
size(soloAlbum)
length(soloAlbum)
length(artistCount)
length(artistCount$album_id)
table(soloAlbum)
soloAlbum <- names(table(soloAlbum))
soloAlbum
dat2[which(dat2$album_id %in% soloAlbum),]
table(soloAlbum) = 1
table(soloAlbum) == 1
soloAlbum <- names(table(soloAlbum) == 1)
length(soloAlbum)
dat2[which(dat2$album_id %in% soloAlbum),]
soloAlbum <-
names(table(soloAlbum))[table(soloAlbum) == 1]
length(soloAlbum)
artistCount <- count(dat2, artists, album_id)
soloAlbum <- artistCount$album_id[artistCount$n > 1]
length(soloAlbum)
soloAlbum <- names(table(soloAlbum) == 1)
length(soloAlbum)
n_distinct(soloAlbum)
dat2[which(dat2$album_id %in% soloAlbum),]
## Removed commas in strings
dat2 <- read.csv("csv/cleaned_tracks.csv")
dat2 <- dat2[!grepl("Live at ", dat2$album, ignore.case=TRUE),]
artistCount <- count(dat2, artists, album_id)
soloAlbum <- artistCount$album_id[artistCount$n > 1]
soloAlbum <- names(table(soloAlbum) == 1)
dat2[which(dat2$album_id %in% soloAlbum),]
dat2 <- dat2[which(dat2$album_id %in% soloAlbum),]
dat2$name <- gsub(",", "", dat2$name)
dat2$album <- gsub(",", "", dat2$album)
dat2$artists <- gsub(",", "", dat2$artists)
dat2 <- dat2[,!(names(dat2) == "X")]
head(dat2)
write.csv(dat2,file="csv/cleaned_tracks.csv")
length(dat2)
length(dat2$id)
names(table(soloAlbum) == 1)
table(soloAlbum)
write.csv(dat2,file="csv/cleaned_tracks.csv")
## Clean Part 2
dat2 <- read.csv("csv/cleaned_tracks.csv")
## Remove compilation albums
artistCount <- count(dat2, artists, album_id)
artistCount
soloAlbum <- artistCount$album_id[artistCount$n > 1]
soloAlbum <- names(table(soloAlbum) == 1)
table(soloAlbum)
sum(table(soloAlbum))
dat2 <- dat2[which(dat2$album_id %in% soloAlbum),]
length(dat2$id)
count(dat2, artists, album_id)
count(dat2, album_id, artists)
albumCount <- count(dat2, album_id, artists)
n_distinct(albumCount)
n_distinct(albumCount$album_id)
table(albumCount$album_id)
names(table(albumCount$album_id) == 1)
length(albumCount$album_id)
soloAlbum <- names(table(albumCount$album_id) == 1)
table(soloAlbum)
sum(table(soloAlbum))
dat2[which(dat2$album_id %in% soloAlbum),]
names(table(albumCount$album_id) != 1)
names(table(albumCount$album_id) == 1)
table(albumCount$album_id) == 1
soloAlbum <- names(table(albumCount$album_id))
soloAlbum[table(albumCount$album_id) == 1]
allAlbums <- names(table(albumCount$album_id))
soloAlbum <- allAlbums[table(albumCount$album_id) == 1]
table(allAlbums)
table(soloAlbum)
dat2 <- dat2[which(dat2$album_id %in% soloAlbum),]
length(dat2$id)
dat2 <- dat2[,!(names(dat2) == "X")]
head(dat2)
write.csv(dat2,file="csv/cleaned_tracks.csv")
length(soloAlbum)
